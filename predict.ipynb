{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Classification Prediction\n",
    "\n",
    "This notebook provides functionality to classify poses as correct or incorrect using our trained GCN model.\n",
    "\n",
    "## Input Requirements\n",
    "- **Keypoints**: Array/list of shape (17, 2) containing x,y coordinates in COCO format\n",
    "- **Visibility** (optional): Array/list of shape (17) containing visibility flags\n",
    "  - 0: not labeled\n",
    "  - 1: labeled but not visible\n",
    "  - 2: labeled and visible (default)\n",
    "\n",
    "### COCO Keypoint Order\n",
    "1. nose\n",
    "2. left_eye\n",
    "3. right_eye\n",
    "4. left_ear\n",
    "5. right_ear\n",
    "6. left_shoulder\n",
    "7. right_shoulder\n",
    "8. left_elbow\n",
    "9. right_elbow\n",
    "10. left_wrist\n",
    "11. right_wrist\n",
    "12. left_hip\n",
    "13. right_hip\n",
    "14. left_knee\n",
    "15. right_knee\n",
    "16. left_ankle\n",
    "17. right_ankle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our modules\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from data_processing import load_dataset, PoseDataset\n",
    "from gcn_model import PoseGCN\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"Load the trained model from a checkpoint file.\"\"\"\n",
    "    # Initialize model\n",
    "    model = PoseGCN(num_node_features=2).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_single_pose_input(keypoints, visibility=None):\n",
    "    \"\"\"Prepare a single pose input for prediction.\"\"\"\n",
    "    # Convert input to numpy array if it's a list\n",
    "    keypoints = np.array(keypoints)\n",
    "    \n",
    "    # Validate input shape\n",
    "    if keypoints.shape != (17, 2):\n",
    "        raise ValueError(f\"Expected keypoints shape (17, 2), got {keypoints.shape}\")\n",
    "    \n",
    "    # Handle visibility\n",
    "    if visibility is None:\n",
    "        visibility = np.full(17, 2)  # All keypoints visible\n",
    "    else:\n",
    "        visibility = np.array(visibility)\n",
    "        if visibility.shape != (17,):\n",
    "            raise ValueError(f\"Expected visibility shape (17,), got {visibility.shape}\")\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    x = torch.tensor(keypoints, dtype=torch.float)\n",
    "    v = torch.tensor(visibility, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Create edge index for the pose graph\n",
    "    edge_index = torch.tensor([\n",
    "        [0, 1], [1, 0],  # nose - left eye\n",
    "        [0, 2], [2, 0],  # nose - right eye\n",
    "        [1, 3], [3, 1],  # left eye - left ear\n",
    "        [2, 4], [4, 2],  # right eye - right ear\n",
    "        [5, 6], [6, 5],  # left shoulder - right shoulder\n",
    "        [5, 7], [7, 5],  # left shoulder - left elbow\n",
    "        [6, 8], [8, 6],  # right shoulder - right elbow\n",
    "        [7, 9], [9, 7],  # left elbow - left wrist\n",
    "        [8, 10], [10, 8],  # right elbow - right wrist\n",
    "        [5, 11], [11, 5],  # left shoulder - left hip\n",
    "        [6, 12], [12, 6],  # right shoulder - right hip\n",
    "        [11, 12], [12, 11],  # left hip - right hip\n",
    "        [11, 13], [13, 11],  # left hip - left knee\n",
    "        [12, 14], [14, 12],  # right hip - right knee\n",
    "        [13, 15], [15, 13],  # left knee - left ankle\n",
    "        [14, 16], [16, 14],  # right knee - right ankle\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), vis=v)\n",
    "    return data\n",
    "\n",
    "def predict_single(model, data, device):\n",
    "    \"\"\"Make prediction for a single pose.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        prob = torch.exp(output)  # Convert log probabilities to probabilities\n",
    "    return pred.item(), prob[0].cpu().numpy()\n",
    "\n",
    "def predict_pose(keypoints, visibility=None):\n",
    "    \"\"\"Predict whether a pose is correct or incorrect.\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the trained model\n",
    "    model_path = 'models/model-1/best_model.pth'\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    \n",
    "    model = load_trained_model(model_path, device)\n",
    "    \n",
    "    # Prepare input data\n",
    "    data = prepare_single_pose_input(keypoints, visibility)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction, probabilities = predict_single(model, data, device)\n",
    "    \n",
    "    # Convert prediction to class name and get confidence\n",
    "    pred_class = \"Correct\" if prediction == 1 else \"Incorrect\"\n",
    "    confidence = max(probabilities)\n",
    "    \n",
    "    return pred_class, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Test on Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading annotations from annotations_v1\n",
      "Loaded 102 annotations from annotations_v1\\correct_n2_3_7_100.json\n",
      "Number of images: 579\n",
      "Categories: ['human']\n",
      "Successfully processed 102 keypoint annotations\n",
      "Loaded 108 annotations from annotations_v1\\lumbar_3_7_dell_111.json\n",
      "Number of images: 464\n",
      "Categories: ['human']\n",
      "Successfully processed 108 keypoint annotations\n",
      "Loaded 102 correct poses and 108 incorrect poses\n",
      "Total dataset size: 210 samples\n",
      "\n",
      "Loaded 42 test samples\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load test dataset\n",
    "_, test_dataset = load_dataset(\n",
    "    root_dir='data_v1',\n",
    "    annotation_dir='annotations_v1'\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data loader and make predictions\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# # Load model\n",
    "# model = load_trained_model('models/model-1/best_model.pth', device)\n",
    "\n",
    "# # Make predictions\n",
    "# model.eval()\n",
    "# predictions = []\n",
    "# probabilities = []\n",
    "# labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in test_loader:\n",
    "#         data = data.to(device)\n",
    "#         output = model(data)\n",
    "#         pred = output.max(dim=1)[1]\n",
    "#         prob = torch.exp(output)\n",
    "        \n",
    "#         predictions.extend(pred.cpu().numpy())\n",
    "#         probabilities.extend(prob.cpu().numpy())\n",
    "#         labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "# # Print classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(labels, predictions, target_names=['Incorrect', 'Correct']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Predict Single Pose\n",
    "\n",
    "Here's how to use the model to predict a single pose. Replace the example keypoints with your actual pose keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m example_visibility \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m17\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pred_class, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_keypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_visibility\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 78\u001b[0m, in \u001b[0;36mpredict_pose\u001b[1;34m(keypoints, visibility)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Prepare input data\u001b[39;00m\n\u001b[0;32m     81\u001b[0m data \u001b[38;5;241m=\u001b[39m prepare_single_pose_input(keypoints, visibility)\n",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mload_trained_model\u001b[1;34m(model_path, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded model from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "# Example keypoints (replace with your actual keypoints)\n",
    "example_keypoints = np.array([\n",
    "    [0.5, 0.5],  # nose\n",
    "    [0.45, 0.45],  # left_eye\n",
    "    [0.55, 0.45],  # right_eye\n",
    "    [0.4, 0.5],  # left_ear\n",
    "    [0.6, 0.5],  # right_ear\n",
    "    [0.3, 0.7],  # left_shoulder\n",
    "    [0.7, 0.7],  # right_shoulder\n",
    "    [0.2, 0.9],  # left_elbow\n",
    "    [0.8, 0.9],  # right_elbow\n",
    "    [0.1, 1.1],  # left_wrist\n",
    "    [0.9, 1.1],  # right_wrist\n",
    "    [0.4, 1.2],  # left_hip\n",
    "    [0.6, 1.2],  # right_hip\n",
    "    [0.35, 1.5],  # left_knee\n",
    "    [0.65, 1.5],  # right_knee\n",
    "    [0.3, 1.8],  # left_ankle\n",
    "    [0.7, 1.8],  # right_ankle\n",
    "])\n",
    "\n",
    "# Example visibility (all keypoints visible)\n",
    "example_visibility = np.ones(17) * 2\n",
    "\n",
    "# Make prediction\n",
    "pred_class, confidence = predict_pose(example_keypoints, example_visibility)\n",
    "\n",
    "print(f\"Prediction: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use for Your Own Poses\n",
    "\n",
    "To classify your own poses:\n",
    "\n",
    "1. Prepare your keypoints in COCO format (17 keypoints, each with x,y coordinates)\n",
    "2. Optionally prepare visibility flags for each keypoint\n",
    "3. Use the `predict_pose()` function as shown in Example 2 above\n",
    "\n",
    "Example code template:\n",
    "```python\n",
    "# Prepare your keypoints\n",
    "my_keypoints = np.array([\n",
    "    [x1, y1],  # nose\n",
    "    [x2, y2],  # left_eye\n",
    "    # ... add all 17 keypoints\n",
    "])\n",
    "\n",
    "# Optional: prepare visibility\n",
    "my_visibility = np.array([2, 2, 2, ...])  # 2 for visible, 1 for occluded, 0 for unlabeled\n",
    "\n",
    "# Make prediction\n",
    "prediction, confidence = predict_pose(my_keypoints, my_visibility)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
