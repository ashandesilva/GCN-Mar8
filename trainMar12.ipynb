{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from annotations_v3/n6-269-299-correct.json\n",
      "Loaded 300 annotations\n",
      "Loading annotations from annotations_v3/lumbar-neth-K-1.1-285-w53445.json\n",
      "Loaded 286 annotations\n",
      "Parsed 300 keypoints with label 1\n",
      "Parsed 286 keypoints with label 0\n",
      "Total data points: 586\n",
      "Training data points: 468, Validation data points: 118\n"
     ]
    }
   ],
   "source": [
    "# Load and parse annotations\n",
    "def load_annotations(annotation_file):\n",
    "    print(f\"Loading annotations from {annotation_file}\")\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Loaded {len(annotations['annotations'])} annotations\")\n",
    "    return annotations\n",
    "\n",
    "def parse_keypoints(annotations, label):\n",
    "    data = []\n",
    "    for ann in annotations['annotations']:\n",
    "        keypoints = ann['keypoints']\n",
    "        keypoints_array = np.array(keypoints).reshape(-1, 3)\n",
    "        coords = keypoints_array[:, :2]\n",
    "        visibility = keypoints_array[:, 2]\n",
    "        data.append((coords, visibility, label))\n",
    "    print(f\"Parsed {len(data)} keypoints with label {label}\")\n",
    "    return data\n",
    "\n",
    "# Load annotations\n",
    "correct_annotations = load_annotations('annotations_v3/n6-269-299-correct.json')\n",
    "incorrect_annotations = load_annotations('annotations_v3/lumbar-neth-K-1.1-285-w53445.json')\n",
    "\n",
    "# Parse keypoints\n",
    "correct_data = parse_keypoints(correct_annotations, label=1)\n",
    "incorrect_data = parse_keypoints(incorrect_annotations, label=0)\n",
    "\n",
    "# Combine and split data\n",
    "all_data = correct_data + incorrect_data\n",
    "print(f\"Total data points: {len(all_data)}\")\n",
    "train_data, val_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "print(f\"Training data points: {len(train_data)}, Validation data points: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Changes:\n",
    "- Edge Index Adjustment: The edge_index is adjusted for batching by offsetting node indices.\n",
    "- Batch Tensor: Used torch.arange to create a batch tensor for the Data object.\n",
    "- Logging: Added print statements for loading annotations, parsing keypoints, and during training to track progress and debug.\n",
    "- Metric Tracking and Plotting: Metrics are tracked and plotted for both models, and a comparison is printed at the end.\n",
    "____\n",
    "- Edge Index Definition: Added edge_index definition in the PoseDataset class.\n",
    "- Batch Handling: Used torch.arange to create a batch tensor for the Data object.\n",
    "- Print Statements: Added print statements for debugging and tracking progress during training.\n",
    "- Metric Tracking: Tracked accuracy, precision, recall, and F1 score for both models.\n",
    "- Plotting: Added functions to plot metrics for visualization.\n",
    "- Model Comparison: Added a section to compare the best performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        coords, visibility, label = self.data[idx]\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)\n",
    "        visibility_tensor = torch.tensor(visibility, dtype=torch.float)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        # Define edge_index for the pose graph\n",
    "        edge_index = torch.tensor([\n",
    "            [0, 1], [1, 0],  # nose - left eye\n",
    "            [0, 2], [2, 0],  # nose - right eye\n",
    "            [1, 3], [3, 1],  # left eye - left ear\n",
    "            [2, 4], [4, 2],  # right eye - right ear\n",
    "            [5, 6], [6, 5],  # left shoulder - right shoulder\n",
    "            [5, 7], [7, 5],  # left shoulder - left elbow\n",
    "            [6, 8], [8, 6],  # right shoulder - right elbow\n",
    "            [7, 9], [9, 7],  # left elbow - left wrist\n",
    "            [8, 10], [10, 8],  # right elbow - right wrist\n",
    "            [5, 11], [11, 5],  # left shoulder - left hip\n",
    "            [6, 12], [12, 6],  # right shoulder - right hip\n",
    "            [11, 12], [12, 11],  # left hip - right hip\n",
    "            [11, 13], [13, 11],  # left hip - left knee\n",
    "            [12, 14], [14, 12],  # right hip - right knee\n",
    "            [13, 15], [15, 13],  # left knee - left ankle\n",
    "            [14, 16], [16, 14],  # right knee - right ankle\n",
    "        ], dtype=torch.long).t().contiguous()\n",
    "        return coords_tensor, visibility_tensor, label_tensor, edge_index\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PoseDataset(train_data)\n",
    "val_dataset = PoseDataset(val_data)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset.data)\n",
    "# print(train_loader.dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the original GCN model\n",
    "class PoseGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features):\n",
    "        super(PoseGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.linear = torch.nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep GCN model\n",
    "class DeepPoseGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels=64, num_classes=2):\n",
    "        super(DeepPoseGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, hidden_channels//2)\n",
    "        self.linear2 = torch.nn.Linear(hidden_channels//2, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        x2 = x2 + x1\n",
    "        x3 = self.conv3(x2, edge_index)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        x3 = x3 + x2\n",
    "        x4 = self.conv4(x3, edge_index)\n",
    "        x = global_mean_pool(x4, data.batch)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for coords, visibility, labels, edge_index in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Adjust edge_index for batching\n",
    "        batch_size = coords.size(0)\n",
    "        num_nodes = coords.size(1)\n",
    "        \n",
    "        # Repeat edge_index for each graph in the batch\n",
    "        batch_edge_index = edge_index.repeat(1, batch_size)\n",
    "        \n",
    "        # Offset node indices for each graph in the batch\n",
    "        node_offset = torch.arange(batch_size, device=device).repeat_interleave(edge_index.size(1)) * num_nodes\n",
    "        batch_edge_index += node_offset.unsqueeze(0)\n",
    "        \n",
    "        # Create Data object\n",
    "        data = Data(x=coords.view(-1, 2).to(device), edge_index=batch_edge_index.to(device), batch=torch.arange(batch_size, device=device).repeat_interleave(num_nodes))\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for coords, visibility, labels_batch, edge_index in loader:\n",
    "            batch_size = coords.size(0)\n",
    "            num_nodes = coords.size(1)\n",
    "            \n",
    "            # Repeat edge_index for each graph in the batch\n",
    "            batch_edge_index = edge_index.repeat(1, batch_size)\n",
    "            \n",
    "            # Offset node indices for each graph in the batch\n",
    "            node_offset = torch.arange(batch_size, device=device).repeat_interleave(edge_index.size(1)) * num_nodes\n",
    "            batch_edge_index += node_offset.unsqueeze(0)\n",
    "            \n",
    "            # Create Data object\n",
    "            data = Data(x=coords.view(-1, 2).to(device), edge_index=batch_edge_index.to(device), batch=torch.arange(batch_size, device=device).repeat_interleave(num_nodes))\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels_batch.to(device))\n",
    "            total_loss += loss.item()\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(labels_batch.to(device)).sum().item()\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            labels.extend(labels_batch.cpu().numpy())\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return total_loss / len(loader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     val_loss, val_acc, val_prec, val_rec, val_f1 \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[0;32m     18\u001b[0m     original_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_acc)\n",
      "Cell \u001b[1;32mIn[78], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Repeat edge_index for each graph in the batch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m batch_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43medge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Offset node indices for each graph in the batch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m node_offset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat_interleave(edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m num_nodes\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 100\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize metrics storage\n",
    "original_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "deep_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "# Train original GCN\n",
    "model = PoseGCN(num_node_features=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Training Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate(model, val_loader, criterion, device)\n",
    "    original_metrics['accuracy'].append(val_acc)\n",
    "    original_metrics['precision'].append(val_prec)\n",
    "    original_metrics['recall'].append(val_rec)\n",
    "    original_metrics['f1'].append(val_f1)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Prec: {val_prec:.4f}, Val Rec: {val_rec:.4f}, Val F1: {val_f1:.4f}')\n",
    "\n",
    "# Train deep GCN\n",
    "deep_model = DeepPoseGCN(num_node_features=2).to(device)\n",
    "deep_optimizer = torch.optim.Adam(deep_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Training Deep Model Epoch {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train(deep_model, train_loader, deep_optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate(deep_model, val_loader, criterion, device)\n",
    "    deep_metrics['accuracy'].append(val_acc)\n",
    "    deep_metrics['precision'].append(val_prec)\n",
    "    deep_metrics['recall'].append(val_rec)\n",
    "    deep_metrics['f1'].append(val_f1)\n",
    "    print(f'Deep Model Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Prec: {val_prec:.4f}, Val Rec: {val_rec:.4f}, Val F1: {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "def plot_metrics(metrics, title, save_path):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    epochs = range(1, len(metrics['accuracy']) + 1)\n",
    "    plt.plot(epochs, metrics['accuracy'], 'g-', label='Accuracy')\n",
    "    plt.plot(epochs, metrics['precision'], 'r-', label='Precision')\n",
    "    plt.plot(epochs, metrics['recall'], 'b-', label='Recall')\n",
    "    plt.plot(epochs, metrics['f1'], 'y-', label='F1 Score')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "# Plot metrics for original model\n",
    "plot_metrics(original_metrics, 'Original PoseGCN Metrics', 'original_posegcn_metrics.png')\n",
    "\n",
    "# Plot metrics for deep model\n",
    "plot_metrics(deep_metrics, 'Deep PoseGCN Metrics', 'deep_posegcn_metrics.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Original PoseGCN - Best F1: {max(original_metrics['f1']):.4f}\")\n",
    "print(f\"DeepPoseGCN - Best F1: {max(deep_metrics['f1']):.4f}\")\n",
    "\n",
    "# Add comparison of best precision, recall, and accuracy values\n",
    "print(\"\\n=== Best Precision, Recall, Accuracy Comparison ===\")\n",
    "print(f\"Original PoseGCN - Best Precision: {max(original_metrics['precision']):.4f}\")\n",
    "print(f\"DeepPoseGCN - Best Precision: {max(deep_metrics['precision']):.4f}\")\n",
    "print(f\"Original PoseGCN - Best Recall: {max(original_metrics['recall']):.4f}\")\n",
    "print(f\"DeepPoseGCN - Best Recall: {max(deep_metrics['recall']):.4f}\")\n",
    "print(f\"Original PoseGCN - Best Accuracy: {max(original_metrics['accuracy']):.4f}\")\n",
    "print(f\"DeepPoseGCN - Best Accuracy: {max(deep_metrics['accuracy']):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
