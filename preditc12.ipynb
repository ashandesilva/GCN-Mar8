{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the original GCN model\n",
    "class PoseGCN(torch.nn.Module):\n",
    "    \"\"\"Original GCN model for pose classification.\"\"\"\n",
    "    def __init__(self, num_node_features):\n",
    "        super(PoseGCN, self).__init__()\n",
    "        # GCN layers\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        \n",
    "        # Classification layers\n",
    "        self.linear = torch.nn.Linear(64, 2)  # 2 classes: correct/incorrect\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (average all node features)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        \n",
    "        # Classification layer\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Define the deep GCN model\n",
    "class DeepPoseGCN(torch.nn.Module):\n",
    "    \"\"\"Enhanced deeper GCN model with residual connections and batch normalization.\"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels=64, num_classes=2):\n",
    "        super(DeepPoseGCN, self).__init__()\n",
    "        \n",
    "        # Multiple GCN layers with residual connections\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        # Final classification layers\n",
    "        self.linear1 = torch.nn.Linear(hidden_channels, hidden_channels//2)\n",
    "        self.linear2 = torch.nn.Linear(hidden_channels//2, num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # First block\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.dropout(x1, p=0.2, training=self.training)\n",
    "        \n",
    "        # Second block with residual connection\n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = F.dropout(x2, p=0.2, training=self.training)\n",
    "        x2 = x2 + x1  # Residual connection\n",
    "        \n",
    "        # Third block with residual connection\n",
    "        x3 = self.conv3(x2, edge_index)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = F.dropout(x3, p=0.2, training=self.training)\n",
    "        x3 = x3 + x2  # Residual connection\n",
    "        \n",
    "        # Fourth block\n",
    "        x4 = self.conv4(x3, edge_index)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x4, data.batch)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_class, model_path, device):\n",
    "    # \"\"\"Load the trained model from a state dict file.\"\"\"\n",
    "    # # Initialize model\n",
    "    # model = model_class(num_node_features=2).to(device)\n",
    "    \n",
    "    # # Load state dict\n",
    "    # state_dict = torch.load(model_path, map_location=device)\n",
    "    # model.load_state_dict(state_dict)\n",
    "    # print(f\"{model_class.__name__} loaded successfully\")\n",
    "    \n",
    "    # return model\n",
    "    \"\"\"Load the trained model from a checkpoint file.\"\"\"\n",
    "    # Initialize model\n",
    "    model = model_class(num_node_features=2).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Extract the model's state dictionary\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint  # Assume it's a direct state dict if key not found\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"{model_class.__name__} loaded successfully\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_single_pose_input(keypoints, visibility=None):\n",
    "    \"\"\"\n",
    "    Prepare a single pose input for prediction.\n",
    "    \n",
    "    Args:\n",
    "        keypoints: numpy array or list of shape [17, 2] containing x,y coordinates\n",
    "        visibility: optional numpy array or list of shape [17] containing visibility flags\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it's a list\n",
    "    keypoints = np.array(keypoints)\n",
    "    \n",
    "    # Validate input shape\n",
    "    if keypoints.shape != (17, 2):\n",
    "        raise ValueError(f\"Expected keypoints shape (17, 2), got {keypoints.shape}\")\n",
    "    \n",
    "    # Handle visibility\n",
    "    if visibility is None:\n",
    "        visibility = np.full(17, 2)  # All keypoints visible\n",
    "    else:\n",
    "        visibility = np.array(visibility)\n",
    "        if visibility.shape != (17,):\n",
    "            raise ValueError(f\"Expected visibility shape (17,), got {visibility.shape}\")\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    x = torch.tensor(keypoints, dtype=torch.float)\n",
    "    v = torch.tensor(visibility, dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    # Create edge index for the pose graph\n",
    "    edge_index = torch.tensor([\n",
    "        [0, 1], [1, 0],  # nose - left eye\n",
    "        [0, 2], [2, 0],  # nose - right eye\n",
    "        [1, 3], [3, 1],  # left eye - left ear\n",
    "        [2, 4], [4, 2],  # right eye - right ear\n",
    "        [5, 6], [6, 5],  # left shoulder - right shoulder\n",
    "        [5, 7], [7, 5],  # left shoulder - left elbow\n",
    "        [6, 8], [8, 6],  # right shoulder - right elbow\n",
    "        [7, 9], [9, 7],  # left elbow - left wrist\n",
    "        [8, 10], [10, 8],  # right elbow - right wrist\n",
    "        [5, 11], [11, 5],  # left shoulder - left hip\n",
    "        [6, 12], [12, 6],  # right shoulder - right hip\n",
    "        [11, 12], [12, 11],  # left hip - right hip\n",
    "        [11, 13], [13, 11],  # left hip - left knee\n",
    "        [12, 14], [14, 12],  # right hip - right knee\n",
    "        [13, 15], [15, 13],  # left knee - left ankle\n",
    "        [14, 16], [16, 14],  # right knee - right ankle\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), vis=v)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(model, data, device):\n",
    "    \"\"\"Make prediction for a single pose.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        prob = torch.exp(output)  # Convert log probabilities to probabilities\n",
    "    return pred.item(), prob[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pose(model_class, model_path, keypoints, visibility=None):\n",
    "    \"\"\"\n",
    "    Predict whether a pose is correct or incorrect using a specified model.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The class of the model to use (PoseGCN or DeepPoseGCN)\n",
    "        model_path: Path to the model's state dict file\n",
    "        keypoints: numpy array or list of shape [17, 2] containing x,y coordinates in COCO format\n",
    "        visibility: optional numpy array or list of shape [17] containing visibility flags\n",
    "    \n",
    "    Returns:\n",
    "        prediction: \"Correct\" or \"Incorrect\"\n",
    "        confidence: confidence score of the prediction\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = load_trained_model(model_class, model_path, device)\n",
    "    \n",
    "    # Prepare input data\n",
    "    data = prepare_single_pose_input(keypoints, visibility)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction, probabilities = predict_single(model, data, device)\n",
    "    \n",
    "    # Convert prediction to class name and get confidence\n",
    "    pred_class = \"Correct\" if prediction == 1 else \"Incorrect\"\n",
    "    confidence = max(probabilities)\n",
    "    \n",
    "    return pred_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict Input - YOLO Pose output\n",
    "# # keypoint = result.keypoints.data[0]\n",
    "# # Example keypoints from the ultralytics object\n",
    "# keypoints_tensor = torch.tensor([[ 25.3896,  44.3985,   0.9876],\n",
    "#         [ 19.5475,  37.6896,   0.9656],\n",
    "#         [ 20.2674,  39.5152,   0.9838],\n",
    "#         [ 25.3273,  26.4775,   0.9327],\n",
    "#         [ 26.9300,  28.1562,   0.9810],\n",
    "#         [ 67.0843,  26.7597,   0.9872],\n",
    "#         [ 68.6909,  30.5649,   0.9859],\n",
    "#         [105.2338,  39.4868,   0.9922],\n",
    "#         [107.9833,  44.5882,   0.9917],\n",
    "#         [ 93.6414,  76.5510,   0.9988],\n",
    "#         [ 94.0114,  83.4342,   0.9986],\n",
    "#         [125.6109,  63.7715,   0.9641],\n",
    "#         [128.7283,  67.5244,   0.9530],\n",
    "#         [ 82.6642, 132.5754,   0.9990],\n",
    "#         [ 83.2223, 141.3123,   0.9987],\n",
    "#         [ 84.7002, 193.8848,   0.9298],\n",
    "#         [ 89.8226, 205.7054,   0.7688]])\n",
    "# # Extract x, y coordinates and visibility\n",
    "# keypoints = keypoints_tensor[:, :2].numpy()\n",
    "# visibility = (keypoints_tensor[:, 2] > 0.5).int().numpy() * 2  # Convert to visibility flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 23.38  45.06]\n",
      " [ 18.12  39.27]\n",
      " [ 18.78  37.79]\n",
      " [ 25.84  18.9 ]\n",
      " [ 28.26  18.24]\n",
      " [ 67.08  68.7 ]\n",
      " [ 77.33  40.  ]\n",
      " [ 78.44 101.45]\n",
      " [ 88.28  85.78]\n",
      " [ 78.23 136.18]\n",
      " [ 85.99 113.58]\n",
      " [132.19  63.82]\n",
      " [150.18  57.63]\n",
      " [ 99.56 147.38]\n",
      " [107.83 143.6 ]\n",
      " [105.16 212.56]\n",
      " [112.12 213.67]]\n",
      "[2. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(keypoints)\n",
    "print(visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoseGCN loaded successfully\n",
      "Original Model Prediction: Correct, Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the original GCN model to classify\n",
    "pred_class_original, confidence_original = predict_pose(PoseGCN, 'models/model-7/original/model_PoseGCN_best.pth', keypoints, visibility)\n",
    "print(f\"Original Model Prediction: {pred_class_original}, Confidence: {confidence_original:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepPoseGCN loaded successfully\n",
      "Deep Model Prediction: Correct, Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the deep GCN model to classify\n",
    "pred_class_deep, confidence_deep = predict_pose(DeepPoseGCN, 'models/model-7/deep/model_DeepPoseGCN_best.pth', keypoints, visibility)\n",
    "print(f\"Deep Model Prediction: {pred_class_deep}, Confidence: {confidence_deep:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "flat_keypoints = [\n",
    "    28.85, 49.25, 2, 23.59, 43.46, 1, 24.26, 41.98, 2, 31.31, 23.08, 1,\n",
    "        33.74, 22.43, 2, 75.53, 49.52, 1, 85.37, 14.73, 1, 100.41, 69.53, 1,\n",
    "        118.81, 22.88, 1, 95.06, 104.4, 1, 103.65, 57.29, 1, 128.42, 64.95, 1,\n",
    "        155.05, 60.26, 1, 98.46, 147.89, 1, 104.67, 148.01, 2, 110.63, 224.03,\n",
    "        1, 115.85, 226.6, 2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keypoints(flat_keypoints):\n",
    "    \"\"\"\n",
    "    Process flat keypoints list into separate arrays for coordinates and visibility.\n",
    "    \n",
    "    Args:\n",
    "        flat_keypoints: List of keypoints in the format [x1, y1, v1, x2, y2, v2, ..., x17, y17, v17]\n",
    "    \n",
    "    Returns:\n",
    "        keypoints: numpy array of shape (17, 2) containing x, y coordinates\n",
    "        visibility: numpy array of shape (17,) containing visibility flags\n",
    "    \"\"\"\n",
    "    # Reshape the flat list into a (17, 3) array\n",
    "    keypoints_array = np.array(flat_keypoints).reshape(-1, 3)\n",
    "    \n",
    "    # Extract x, y coordinates\n",
    "    keypoints = keypoints_array[:, :2]\n",
    "    \n",
    "    # Extract visibility flags\n",
    "    visibility = keypoints_array[:, 2]\n",
    "    \n",
    "    return keypoints, visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints, visibility = process_keypoints(flat_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoseGCN loaded successfully\n",
      "Prediction: Correct, Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Now you can use these keypoints and visibility with the predict_pose function\n",
    "pred_class, confidence = predict_pose(PoseGCN, 'models/model-7/original/model_PoseGCN_best.pth', keypoints, visibility)\n",
    "print(f\"Prediction: {pred_class}, Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepPoseGCN loaded successfully"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep Model Prediction: Correct, Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the deep GCN model to classify\n",
    "pred_class_deep, confidence_deep = predict_pose(DeepPoseGCN, 'models/model-7/deep/model_DeepPoseGCN_best.pth', keypoints, visibility)\n",
    "print(f\"Deep Model Prediction: {pred_class_deep}, Confidence: {confidence_deep:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
